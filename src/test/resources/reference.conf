codefeedr {

  mongo {
    host = "localhost"
    port = 27017
    username = ""
    password = ""
    db = "codefeedr_test"
  }

  kafka {
    server {
      bootstrap.servers = "127.0.0.01:9092"
      retries = 0
    }
    consumer {
      bootstrap.servers = "127.0.0.1:9092"
      auto.offset.reset = "earliest"
      auto.commit.interval.ms = 100
      enable.auto.commit = false
    }
    # Kafka configuration specific for codefeedrs kafka connectors
    custom {
      # Number of partitions used when creating kafka topics
      # Each source in Flink can expose data from one or more partitions,
      # so keep flink's parallism divisable by this number
      partition.count = 4
      # The amount of producers each instance of each kafka sink should maintain
      # For each uncommitted checkpoint each instance of each kafka sink maintains a producer
      # 5 is the same used in flinks native kafka source implementation
      producer.count = 5
    }
  }
  zookeeper {
    connectionstring = "127.0.0.1:2181"
    connectTimeout = 5
    sessionTimeout = 30

  }
  flink {
    remote {
      host = "127.0.0.1"
      port = 6123
      parallelism = 1
      jars = "target/scala-2.11/codefeedr_2.11-0.1-SNAPSHOT-tests.jar,target/scala-2.11/codefeedr_2.11-0.1-SNAPSHOT.jar,C:\\Users\\Niels\\.ivy2\\cache\\com.typesafe.scala-logging\\scala-logging_2.11\\bundles\\scala-logging_2.11-3.5.0.jar,C:\\Users\\Niels\\.ivy2\\cache\\org.apache.kafka\\kafka-clients\\jars\\kafka-clients-0.11.0.0.jar"
    }
  }

  input {
    github {
      # Api key for querying GitHub events
      apikeys = ["0", "1", "2"]

      keys = [
        {
          key = "example_key"
          limit = 5000 #per hour
        },
        {
          key = "example_key2"
          limit = 3000 #per hour
        }
      ]

      # Name of the collection to use for events
      events_collection = "github_events"

      # Name of the collection to use for commits
      commits_collection = "github_commits"
    }
  }

}